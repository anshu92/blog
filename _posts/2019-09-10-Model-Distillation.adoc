= Model Distillation
:hp-image: https://github.com/anshu92/blog/raw/gh-pages/images/carolien-van-oijen-GRlRHqEqZTc-unsplash.jpg
:published_at: 2019-09-10
:hp-tags: distillation, machine learning

[.lead]
*How most of the information in large models can be transferred to a small model using soft targets, and what that says about the relationship of information to the dynamics of learning.*

image::https://github.com/anshu92/blog/raw/gh-pages/images/carolien-van-oijen-GRlRHqEqZTc-unsplash.jpg[Bee]

'''
.Recent NLP language models
[width="50%",cols="<,<",frame="all",grid="all"]
|===
|*model*
|*parameters*

|BERT-base
|110 million

|BERT-large
|340 million

|Facebook XLM
|665 million

|OpenAI GPT-2
|774 million
|===
'''

The upward trend of model sizes raises hurdles for the application of these models in the shape of computation and scalability, and requires decisions on the trade-off between accuracy, potency and deployability. Following reading http://www.nlp.town/blog/distilling-bert/[this blog post], I decided to try to summarize (distill) the concept of model distillation, and talk about why this struck me as such an interesting idea.

image::https://github.com/anshu92/blog/raw/gh-pages/images/distill.png[Model Distillation]

As shown above, the method of distillation relies on the idea of *_soft targets_* and *_softmax temperature_*.

image::https://github.com/anshu92/blog/raw/gh-pages/images/tempsoftmax.png[Model Distillation]

