= Model Distillation
:hp-image: /covers/cover.png
:published_at: 2019-09-10
:hp-tags: distillation, machine learning

[.lead]
How most of the information in large models can be transferred to a small model using soft targets, and what that says about the relationship of information to the dynamics of learning.

.Recent NLP language models
|===
|model |parameters
|BERT-base |110 million
|BERT-large |340 million
|Facebook XLM |665 million
|OpenAI GPT-2 |774 million
|===