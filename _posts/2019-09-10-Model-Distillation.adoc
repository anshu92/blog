= Model Distillation
:hp-image: https://github.com/anshu92/blog/raw/gh-pages/images/carolien-van-oijen-GRlRHqEqZTc-unsplash.jpg
:published_at: 2019-09-10
:hp-tags: distillation, machine learning

[.lead]
*How most of the information in large models can be transferred to a small model using soft targets, and what that says about the relationship of information to the dynamics of learning.*

image::https://github.com/anshu92/blog/raw/gh-pages/images/carolien-van-oijen-GRlRHqEqZTc-unsplash.jpg[Bee]

'''
.Recent NLP language models
[width="50%",cols="<,<",frame="all",grid="all"]
|===
|*model*
|*parameters*

|BERT-base
|110 million

|BERT-large
|340 million

|Facebook XLM
|665 million

|OpenAI GPT-2
|774 million
|===
'''

The upward trend of model sizes raises hurdles for the application of these models in the shape of computation and scalability, and requires decisions on the trade-off between accuracy, potency and deployability. Following reading http://www.nlp.town/blog/distilling-bert/[this blog post], I decided to try to summarize (distill) the concept of model distillation, and talk about why this struck me as such an interesting idea.

Most machine learning competitions are won by ensemble of models that each understand the training set in a different way. They are often made as different(architectures, initializations, subsets of data) as possible to minimize correlations between their errors. It is also useful that each of these models should overfit on the data individually and the geometric/arithmetic mean of their predictions is used as the inference output. Although useful when processing time is not a hindrance (like while producing an output for a non-real time competition), ensembles are impractical in most real world test/production cases. They also encode very little knowledge per param which is not ideal for an efficient model.

## Can we mine the knowledge in a training set by collecting a pile of dirt before filtering it for gold nuggets, i.e, can we transfer knowledge learnt by an ensemble of models into a single efficient one?

In order to present the plausability of delinking knowledge with the learned  model parameters, an analogy involving insects can be used. That is, the case where some insects in nature that optimize based on their stage-of-life requirement - larval forms to extract nutrition from environment and an adult form for travelling and reproduction. Contrastingly - cumbersome, large and often ensemble models remain the same during training and deployment. As shown in Fig 1., it should be possible to use larger, more diverse models to learn from training data and collapse that information into a more efficient, smaller model. Another assumption made is that the optimizing an objective function for the training set is close to optimizing the user's true objective - _generalizing well to unseen data._

'''
.Analogy. graphic by Anshuman Sahoo
image::https://github.com/anshu92/blog/raw/gh-pages/images/distill1.png[Analogy]

'''

.Model Distillation. graphic by Anshuman Sahoo
image::https://github.com/anshu92/blog/raw/gh-pages/images/distill.png[Model Distillation]

'''

## Hard vs. Soft Targets

https://arxiv.org/pdf/1503.02531.pdf[Hinton et al., 2015] introduced the concept of *"softmax temperature"*(Fig 3). The probability p~i~ of class i is calculated from the logits z. T is the temperature parameter - as you can see, setting T = 1 gives us the standard softmax function. In the paper, they also found that if the ground truth labels are available, it improves the model if the loss to optimize includes training on ground truth labels in addition to the soft target outputs of the teacher model(as shown in the figure, student loss is added to distillation loss).

'''
.Softmax Function with Temperature
image::https://github.com/anshu92/blog/raw/gh-pages/images/tempsoftmax.png[Model Distillation,align="center"]

[quote, Hinton et al., 2015]
____
When the soft targets have high entropy, they provide much more information per training case than hard targets and much less variance in the gradient between training cases, so the small model can often be trained on much
less data than the original cumbersome model and using a much higher learning rate.
____

.hard vs soft targets
image::https://github.com/anshu92/blog/raw/gh-pages/images/distill3.png[Targets,align="center"]

'''

As you can see in Fig 4., whereas hard targets encode the label of the correct class, soft targets encode a lot more knowledge. For instance, the fact that a cow is 1000 times closer to a dog than a car is captured by the high temperature softmax output of the ensemble.


In experiments involving MNIST, a smaller model of fully connected layers 784x800x800x10 produces 146 test errors. A larger model of fully connected layers 784x1200x1200x10 when trained using dropout, weight constraints and input jitter achieves 67 test errors. When the smaller model is trained on the soft output of the larger model, it achieves an error score of 74 errors. It achieves this without the need for the regularizing steps like dropout, jitter etc. This shows almost all the "dark" knowledge that helps the larger model generalize is successfully passed on to the smaller model. For instance, the larger model learns a similarity metric for the digit classes which helps the smaller model distinguish between them much easier.

Another surprising result from MNIST is that if one class, say 3, is left out of the training set, and the smaller model is trained on the outputs of the larger. If we then adjust the bias of class 3 (this is needed because the smaller model has never seen 3, and assumes it to be improbable), we can actually get 98.6% of the test cases for 3 by the knowledge we learn about it from the other classes. In a similar way, if we only train on 7s and 8s, and lower their biases appropriately, the smaller model achieves 87% across all 10 classes!

Setting the limit for temperature in the soft target formula is equivalent to matching logits as in the work of https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf[Caruana et al., 2006].

'''
.Matching logits as a special case of distillation
image::https://github.com/anshu92/blog/raw/gh-pages/images/distill4.png[Matching logits,align="center"]

'''

If your student model appears to be too small to learn the knowledge of a larger model, it is better to set a lower temperature because you would rather learn the harder targets before you learn the darker information captured by softer targets at higher temperatures.

## Regularizing specialists

The problem with highly specialized models is that they tend to be very over fitted to the training dataset. Our goal hence is to regularize this model without losing their effectiveness on the specialized domain. Often we freeze the first few layers of a specialist model while transferring it to a slightly different dataset. This doesnt always work well because the first few filters of the network might need to be slightly adapted to the new dataset. The solution might be instead to use a mixture of specialists and subsequently distill their knowledge into a smaller efficient model, or using tricks like this:

1. Train specialist models on a subset of classes and keep one dustbin class for all the classes it doesn't specialize in. The specialist should learn two things - a) is this sample in my specialized subset? b) the relative probabilities of all the classes in the subset.
2. After training, allow adjustment of the dustbin class logit to account for data enrichment. Ex. if 50% of samples are in the dustbin class in the training data while in real world they are 99%, we can adjust the logits with an additional log(50) or so to account for this.
3. Initialize with the specialist model with weights of a previously trained generalist model, use early stopping to prevent overfitting. Train on specialist classes.

Note: When combining specialist models, each with its own dustbin class, the model is not making a claim about the fact that all dustbin classes are equally probable but about the sum of probabilities of all dustbin classes.


## References

Model Compression by Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil, 2006. https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf[link]

Distilling the Knowledge in a Neural Network by Geoffrey Hinton, Oriol Vinyals and Jeff Dean, 2015. https://arxiv.org/pdf/1503.02531.pdf[link]

https://www.youtube.com/watch?v=EK61htlw8hY[Video lecture] by Hinton

