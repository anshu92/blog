<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <title>Model Distillation - hallucinations</title>

    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <meta name="description" content="">

    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Model Distillation">
    <meta name="twitter:description" content="">

    <meta property="og:type" content="article">
    <meta property="og:title" content="Model Distillation">
    <meta property="og:description" content="">

    <link href="/favicon.ico" rel="shortcut icon" type="image/x-icon">
    <link href="/apple-touch-icon-precomposed.png" rel="apple-touch-icon">

    <link rel="stylesheet" type="text/css" href="//anshu92.github.io/blog/themes/uno/assets/css/uno.css?v=1568691474687" />

    <link rel="canonical" href="https://anshu92.github.io/blog/2019/09/10/Model-Distillation.html" />
    <meta name="referrer" content="origin" />
    
    <meta property="og:site_name" content="hallucinations" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Model Distillation" />
    <meta property="og:description" content="How most of the information in large models can be transferred to a small model using soft targets, and what that says about the relationship of information to the dynamics of learning. Table 1. Recent NLP language models model parameters BERT-base 110 million BERT-large 340 million Facebook XLM 665 million" />
    <meta property="og:url" content="https://anshu92.github.io/blog/2019/09/10/Model-Distillation.html" />
    <meta property="og:image" content="https://github.com/anshu92/blog/raw/gh-pages/images/carolien-van-oijen-GRlRHqEqZTc-unsplash.jpg" />
    <meta property="article:published_time" content="2019-09-10T00:00:00.000Z" />
    <meta property="article:tag" content="distillation" />
    <meta property="article:tag" content="machine learning" />
    
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Model Distillation" />
    <meta name="twitter:description" content="How most of the information in large models can be transferred to a small model using soft targets, and what that says about the relationship of information to the dynamics of learning. Table 1. Recent NLP language models model parameters BERT-base 110 million BERT-large 340 million Facebook XLM 665 million" />
    <meta name="twitter:url" content="https://anshu92.github.io/blog/2019/09/10/Model-Distillation.html" />
    <meta name="twitter:image:src" content="https://github.com/anshu92/blog/raw/gh-pages/images/carolien-van-oijen-GRlRHqEqZTc-unsplash.jpg" />
    
    <script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "Article",
    "publisher": "hallucinations",
    "author": {
        "@type": "Person",
        "name": "Anshuman Sahoo",
        "image": "https://avatars0.githubusercontent.com/u/3633052?v=4",
        "url": "https://anshu92.github.io/blog/author/anshu92/"
    },
    "headline": "Model Distillation",
    "url": "https://anshu92.github.io/blog/2019/09/10/Model-Distillation.html",
    "datePublished": "2019-09-10T00:00:00.000Z",
    "image": "https://github.com/anshu92/blog/raw/gh-pages/images/carolien-van-oijen-GRlRHqEqZTc-unsplash.jpg",
    "keywords": "distillation, machine learning",
    "description": "How most of the information in large models can be transferred to a small model using soft targets, and what that says about the relationship of information to the dynamics of learning. Table 1. Recent NLP language models model parameters BERT-base 110 million BERT-large 340 million Facebook XLM 665 million"
}
    </script>

    <meta name="generator" content="HubPress" />
    <link rel="alternate" type="application/rss+xml" title="hallucinations" href="https://anshu92.github.io/blog/rss/" />
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/prism/1.14.0/themes/prism-okaidia.min.css">
    
        <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

</head>
<body class="post-template tag-distillation tag-machine-learning no-js">

    <span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    <header class="panel-cover panel-cover--collapsed " style="background-image: url(/blog/images/cover/cover.jpg)">
      <div class="panel-main">
    
        <div class="panel-main__inner panel-inverted">
        <div class="panel-main__content">
    
            <a href="https://anshu92.github.io/blog" title="link to homepage for hallucinations"><img src="/blog/images/profile.jpg" width="80" alt="hallucinations logo" class="panel-cover__logo logo" /></a>
            <h1 class="panel-cover__title panel-title"><a href="https://anshu92.github.io/blog" title="link to homepage for hallucinations">hallucinations</a></h1>
            <hr class="panel-cover__divider" />
            <p class="panel-cover__description">by anshuman sahoo</p>
            <hr class="panel-cover__divider panel-cover__divider--secondary" />
    
            <div class="navigation-wrapper">
    
              <nav class="cover-navigation cover-navigation--primary">
                <ul class="navigation">
                  <li class="navigation__item"><a href="https://anshu92.github.io/blog/#blog" title="link to hallucinations blog" class="blog-button">Blog</a></li>
                </ul>
              </nav>
    
              
              
              <nav class="cover-navigation navigation--social">
                <ul class="navigation">
              
              
              
              
                  <!-- Github -->
                  <li class="navigation__item">
                    <a href="https://github.com/anshu92" title="Github account">
                      <i class='icon icon-social-github'></i>
                      <span class="label">Github</span>
                    </a>
                  </li>
                  </li>
              
              
              
              
                  <!-- LinkedIn -->
                  <li class="navigation__item">
                    <a href="https://ca.linkedin.com/in/anshuman-sahoo-60386b59" title="LinkedIn account">
                      <i class='icon icon-social-linkedin'></i>
                      <span class="label">LinkedIn</span>
                    </a>
                  </li>
              
                  <!-- Email -->
                  <li class="navigation__item">
                    <a href="mailto:anshuman264@gmail.com" title="Email anshuman264@gmail.com">
                      <i class='icon icon-mail'></i>
                      <span class="label">Email</span>
                    </a>
                  </li>
              
                </ul>
              </nav>
              
    
            </div>
    
          </div>
    
        </div>
    
        <div class="panel-cover--overlay"></div>
      </div>
    </header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            

  <article class="post-container post-container--single">

    <header class="post-header">
      <div class="post-meta">
        <time datetime="10 Sep 2019" class="post-meta__date date">10 Sep 2019</time> &#8226; <span class="post-meta__tags tags">on <a href="https://anshu92.github.io/blog/tag/distillation/">distillation</a>, <a href="https://anshu92.github.io/blog/tag/machine-learning/">machine learning</a></span>
        <span class="post-meta__author author"><img src="https://avatars0.githubusercontent.com/u/3633052?v&#x3D;4" alt="profile image for Anshuman Sahoo" class="avatar post-meta__avatar" /> by Anshuman Sahoo</span>
      </div>
      <h1 class="post-title">Model Distillation</h1>
    </header>

    <section class="post tag-distillation tag-machine-learning">
      <div id="preamble">
<div class="sectionbody">
<div class="paragraph lead">
<p><strong>How most of the information in large models can be transferred to a small model using soft targets, and what that says about the relationship of information to the dynamics of learning.</strong></p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/carolien-van-oijen-GRlRHqEqZTc-unsplash.jpg" alt="Bee">
</div>
</div>
<hr>
<table class="tableblock frame-all grid-all" style="width: 50%;">
<caption class="title">Table 1. Recent NLP language models</caption>
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>model</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>parameters</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">BERT-base</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">110 million</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">BERT-large</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">340 million</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Facebook XLM</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">665 million</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">OpenAI GPT-2</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">774 million</p></td>
</tr>
</tbody>
</table>
<hr>
<div class="paragraph">
<p>The upward trend of model sizes raises hurdles for the application of these models, usually in the form of computation and scalability. It requires decisions on the trade-off between accuracy, potency and deployability. Following reading <a href="http://www.nlp.town/blog/distilling-bert/">this blog post</a>, I decided to try to summarize (distill) the concept of model distillation, and talk about why this struck me as such an interesting idea.</p>
</div>
<div class="paragraph">
<p>Most machine learning competitions are won by ensemble of models that each understand the training set in a different way. They are often made as different(architectures, initializations, subsets of data) as possible to minimize correlations between their errors. It is also useful that each of these models should overfit on the data individually and the geometric/arithmetic mean of their predictions is used as the output. Although useful when processing time is not a hindrance (like while producing an output for a non-realtime competition), ensembles are impractical in most real world test/production applications. They also encode very little knowledge per parameter which is not ideal for an efficient model.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_can_we_mine_the_knowledge_in_a_training_set_by_collecting_a_pile_of_dirt_before_filtering_it_for_gold_nuggets_i_e_can_we_transfer_knowledge_learnt_by_an_ensemble_of_models_into_a_single_efficient_one">Can we mine the knowledge in a training set by collecting a pile of dirt before filtering it for gold nuggets, i.e, can we transfer knowledge learnt by an ensemble of models into a single efficient one?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In order to present the plausability of delinking knowledge from the learned  model parameters, an analogy involving insects can be used - the case where some insects in nature that optimize based on their stage-of-life requirement; larval forms to extract nutrition from environment and an adult form for travelling and reproduction. Contrastingly - cumbersome, large and often ensemble models remain the same during training and deployment. As shown in Fig 1., it should be possible to use larger, more diverse models to learn from training data and collapse that information into a more efficient, smaller model. Another assumption made is that the optimizing an objective function for the training set is close to optimizing the user&#8217;s true objective - <em>generalizing well to unseen data.</em> This is not generally the case in modern datasets where the captured data variance is often very different from unseen samples.</p>
</div>
<hr>
<div class="imageblock">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/distill1.png" alt="Analogy">
</div>
<div class="title">Figure 1. Analogy.</div>
</div>
<hr>
<div class="imageblock">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/distill.png" alt="Model Distillation">
</div>
<div class="title">Figure 2. Model Distillation.</div>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_hard_vs_soft_targets">Hard vs. Soft Targets</h2>
<div class="sectionbody">
<div class="paragraph">
<p><a href="https://arxiv.org/pdf/1503.02531.pdf">Hinton et al., 2015</a> introduced the concept of <strong>"softmax temperature"</strong>(Fig 3). The probability p<sub>i</sub> of class i is calculated from the logits z. T is the temperature parameter - as you can see, setting T = 1 gives us the standard softmax function. In the paper, they also found that if the ground truth labels are available, it improves the model if the loss to optimize includes training on ground truth labels in addition to the soft target outputs of the teacher model(as shown in the figure, student loss is added to distillation loss).</p>
</div>
<hr>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/tempsoftmax.png" alt="Model Distillation">
</div>
<div class="title">Figure 3. Softmax Function with Temperature</div>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>When the soft targets have high entropy, they provide much more information per training case than hard targets and much less variance in the gradient between training cases, so the small model can often be trained on much
less data than the original cumbersome model and using a much higher learning rate.</p>
</div>
</blockquote>
<div class="attribution">
&#8212; Hinton et al.<br>
<cite>2015</cite>
</div>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/distill3.png" alt="Targets">
</div>
<div class="title">Figure 4. hard vs soft targets</div>
</div>
<hr>
<div class="paragraph">
<p>As you can see in Fig 4., whereas hard targets encode the label of the correct class, soft targets encode a lot more knowledge. For instance, the fact that a cow is 1000 times closer to a dog than a car is captured by the high temperature softmax output of the ensemble.</p>
</div>
<div class="paragraph">
<p>In experiments involving MNIST, a smaller model of fully connected layers 784x800x800x10 produces 146 test errors. A larger model of fully connected layers 784x1200x1200x10 when trained using dropout, weight constraints and input jitter achieves 67 test errors. When the smaller model is trained on the soft output of the larger model, it achieves an error score of 74 errors. It achieves this without the need for the regularizing steps like dropout, jitter etc. This shows almost all the "dark" knowledge that helps the larger model generalize is successfully passed on to the smaller model. For instance, the larger model learns a similarity metric for the digit classes which helps the smaller model distinguish between them much easier.</p>
</div>
<div class="paragraph">
<p>Another surprising result from MNIST is that if one class, say 3, is left out of the training set, and the smaller model is trained on the outputs of the larger. If we then adjust the bias of class 3 (this is needed because the smaller model has never seen 3, and assumes it to be improbable), we can actually get 98.6% of the test cases for 3 by the knowledge we learn about it from the other classes. In a similar way, if we only train on 7s and 8s, and lower their biases appropriately, the smaller model achieves 87% across all 10 classes!</p>
</div>
<div class="paragraph">
<p>Setting the limit for temperature in the soft target formula is equivalent to matching logits as in the work of <a href="https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf">Caruana et al., 2006</a>.</p>
</div>
<hr>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/distill4.png" alt="Matching logits">
</div>
<div class="title">Figure 5. Matching logits as a special case of distillation</div>
</div>
<hr>
<div class="paragraph">
<p>If your student model appears to be too small to learn the knowledge of a larger model, it is better to set a lower temperature because you would rather learn the harder targets before you learn the darker information captured by softer targets at higher temperatures.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_regularizing_specialists">Regularizing specialists</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The problem with highly specialized models is that they tend to be very over fitted to the training dataset. Our goal hence is to regularize this model without losing their effectiveness on the specialized domain. Often we freeze the first few layers of a large model while transferring it to a slightly different dataset. This doesnt always work well because the first few filters of the network might need to be slightly adapted to the new dataset. The solution might be instead to use a mixture of specialists and subsequently distill their knowledge into a smaller efficient model, or using tricks like this:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Train specialist models on a subset of classes and keep one dustbin class for all the classes it doesn&#8217;t specialize in. The specialist should learn two things - a) is this sample in my specialized subset? b) the relative probabilities of all the classes in the subset.</p>
</li>
<li>
<p>After training, allow adjustment of the dustbin class logit to account for data enrichment. Ex. if 50% of samples are in the dustbin class in the training data while in real world they are 99%, we can adjust the logits with an additional log(50) or so to account for this.</p>
</li>
<li>
<p>Initialize with the specialist model with weights of a previously trained generalist model, use early stopping to prevent overfitting. Train on specialist classes.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Note: When combining specialist models, each with its own dustbin class, the model is not making a claim about the fact that all dustbin classes are equally probable but about the sum of probabilities of all dustbin classes.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_references">References</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Model Compression by Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil, 2006. <a href="https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf">link</a></p>
</div>
<div class="paragraph">
<p>Distilling the Knowledge in a Neural Network by Geoffrey Hinton, Oriol Vinyals and Jeff Dean, 2015. <a href="https://arxiv.org/pdf/1503.02531.pdf">link</a></p>
</div>
<div class="paragraph">
<p><a href="https://www.youtube.com/watch?v=EK61htlw8hY">Video lecture</a> by Hinton</p>
</div>
<div class="paragraph">
<p>Distilling BERT Models with spaCy by Yves Peirsman, 2019. <a href="http://www.nlp.town/blog/distilling-bert/">link</a></p>
</div>
</div>
</div>
    </section>

  </article>




            <footer class="footer">
                <span class="footer__copyright">&copy; 2019. All rights reserved.</span>
                <span class="footer__copyright"><a href="http://uno.daleanthony.com" title="link to page for Uno Ghost theme">Uno theme</a> by <a href="http://daleanthony.com" title="link to website for Dale-Anthony">Dale-Anthony</a></span>
                <span class="footer__copyright">Proudly published with <a href="https://hubpress.github.io" title="link to Hubpress website">Hubpress</a></span>
            </footer>
        </div>
    </div>

    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>

    <script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.9.0/moment-with-locales.min.js?v="></script> <script src="//cdnjs.cloudflare.com/ajax/libs/prism/1.14.0/prism.min.js?v="></script> 
      <script type="text/javascript">
        jQuery( document ).ready(function() {
          // change date with ago
          jQuery('ago.ago').each(function(){
            var element = jQuery(this).parent();
            element.html( moment(element.text()).fromNow());
          });
        });

        // hljs.initHighlightingOnLoad();
      </script>

    <script type="text/javascript" src="//anshu92.github.io/blog/themes/uno/assets/js/main.js?v=1568691474687"></script>

    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-90558314-1', 'auto');
    ga('send', 'pageview');

    </script>

</body>
</html>
