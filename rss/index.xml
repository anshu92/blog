<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[hallucinations]]></title><description><![CDATA[by anshuman sahoo]]></description><link>https://anshu92.github.io/blog</link><image><url>/blog/images/cover/cover.jpg</url><title>hallucinations</title><link>https://anshu92.github.io/blog</link></image><generator>RSS for Node</generator><lastBuildDate>Tue, 17 Sep 2019 02:44:06 GMT</lastBuildDate><atom:link href="https://anshu92.github.io/blog/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Model Distillation]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph lead">
<p><strong>How most of the information in large models can be transferred to a small model using soft targets, and what that says about the relationship of information to the dynamics of learning.</strong></p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/carolien-van-oijen-GRlRHqEqZTc-unsplash.jpg" alt="Bee">
</div>
</div>
<hr>
<table class="tableblock frame-all grid-all" style="width: 50%;">
<caption class="title">Table 1. Recent NLP language models</caption>
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>model</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>parameters</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">BERT-base</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">110 million</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">BERT-large</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">340 million</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Facebook XLM</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">665 million</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">OpenAI GPT-2</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">774 million</p></td>
</tr>
</tbody>
</table>
<hr>
<div class="paragraph">
<p>The upward trend of model sizes raises hurdles for the application of these models in the shape of computation and scalability, and requires decisions on the trade-off between accuracy, potency and deployability. Following reading <a href="http://www.nlp.town/blog/distilling-bert/">this blog post</a>, I decided to try to summarize (distill) the concept of model distillation, and talk about why this struck me as such an interesting idea.</p>
</div>
<div class="paragraph">
<p>Most machine learning competitions are won by ensemble of models that each understand the training set in a different way. They are often made as different(architectures, initializations, subsets of data) as possible to minimize correlations between their errors. It is also useful that each of these models should overfit on the data. Although useful when processing time is not a hindrance (like while producing an output for a non-real time competition), ensembles are impractical in most real world test/production times. They also encode very little knowledge per param which is not ideal for an efficient model.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_can_we_mine_the_knowledge_in_a_training_set_by_collecting_a_pile_of_dirt_before_filtering_it_for_gold_nuggets_i_e_can_we_transfer_knowledge_learnt_by_an_ensemble_of_models_into_a_single_efficient_one">Can we mine the knowledge in a training set by collecting a pile of dirt before filtering it for gold nuggets, i.e, can we transfer knowledge learnt by an ensemble of models into a single efficient one?</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In order to present the plausability of delinking knowledge with the learned  model parameters, an analogy involving insects can be used. That is, the case where some insects in nature that optimize based on their stage-of-life requirement - larval forms to extract nutrition from environment and an adult form for travelling and reproduction. On the other hand - cumbersome, large and often ensemble models remain the same during training and deployment. Another assumption made is that the optimizing an objective function for the training set is close to optimizing the user&#8217;s true objective - <em>generalizing well to unseen data.</em></p>
</div>
<hr>
<div class="imageblock">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/distill1.png" alt="Analogy">
</div>
<div class="title">Figure 1. Analogy. graphic by Anshuman Sahoo</div>
</div>
<hr>
<div class="imageblock">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/distill.png" alt="Model Distillation">
</div>
<div class="title">Figure 2. Model Distillation. graphic by Anshuman Sahoo</div>
</div>
<hr>
</div>
</div>
<div class="sect1">
<h2 id="_hard_vs_soft_targets">Hard vs. Soft Targets</h2>
<div class="sectionbody">
<div class="paragraph">
<p><a href="https://arxiv.org/pdf/1503.02531.pdf">Hinton et al., 2015</a> introduced the concept of <strong>"softmax temperature"</strong>(Fig1). The probability p<sub>i</sub> of class i is calculated from the logits z. T is the temperature parameter - as you can see, setting T = 1 gives us the standard softmax function. In the paper, they also found that if the ground truth labels are available, it improves the model if the loss to optimize includes training on ground truth labels in addition to the soft target outputs of the teacher model(as shown in the figure, student loss is added to distillation loss).</p>
</div>
<hr>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/tempsoftmax.png" alt="Model Distillation">
</div>
<div class="title">Figure 3. Softmax Function with Temperature</div>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>When the soft targets have high entropy, they provide much more information per training case than hard targets and much less variance in the gradient between training cases, so the small model can often be trained on much
less data than the original cumbersome model and using a much higher learning rate.</p>
</div>
</blockquote>
<div class="attribution">
&#8212; Hinton et al.<br>
<cite>2015</cite>
</div>
</div>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/distill3.png" alt="Targets">
</div>
<div class="title">Figure 4. hard vs soft targets</div>
</div>
<hr>
<div class="paragraph">
<p>As you can see in Fig 3., whereas hard targets encode the label of the correct class, soft targets encode a lot more knowledge. For instance, the fact that a cow is 1000 times closer to a dog than a car is captured by the high temperature softmax output of the ensemble.</p>
</div>
<div class="paragraph">
<p>In experiments involving MNIST, a smaller model of fully connected layers 784x800x800x10 produces 146 test errors. A larger model of fully connected layers 784x1200x1200x10 when trained using dropout, weight constraints and input jitter achieves 67 test errors. When the smaller model is trained on the soft output of the larger model, it achieves an error score of 74 errors. It achieves this without the need for the regularizing steps like dropout, jitter etc. This shows almost all the "dark" knowledge that helps the larger model generalize is successfully passed on to the smaller model. For instance, the larger model learns a similarity metric for the digit classes which helps the smaller model distinguish between them much easier.</p>
</div>
<div class="paragraph">
<p>Another surprising result from MNIST is that if one class, say 3, is left out of the training set, and the smaller model is trained on the outputs of the larger. If we then adjust the bias of class 3 (this is needed because the smaller model has never seen 3, and assumes it to be improbable), we can actually get 98.6% of the test cases for 3 by the knowledge we learn about it from the other classes. In a similar way, if we only train on 7s and 8s, and lower their biases appropriately, the smaller model achieves 87% across all 10 classes!</p>
</div>
<div class="paragraph">
<p>Setting the limit for temperature in the soft target formula is equivalent to matching logits as in the work of <a href="https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf">Caruana et al., 2006</a>.</p>
</div>
<hr>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/distill4.png" alt="Matching logits">
</div>
<div class="title">Figure 5. Matching logits as a special case of distillation</div>
</div>
<hr>
<div class="paragraph">
<p>If your student model appears to be too small to learn the knowledge of a larger model, it is better to set a lower temperature because you would rather learn the harder targets before you learn the darker information captured by softer targets at higher temperatures.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_regularizing_specialists">Regularizing specialists</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The problem with highly specialized models is that they tend to be very over fitted to the training dataset. Our goal hence is to regularize this model without losing their effectiveness on the specialized domain. Often we freeze the first few layers of a specialist model while transferring it to a slightly different dataset. This doesnt always work well because the first few filters of the network might need to be slightly adapted to the new dataset. The solution might be instead to use a mixture of specialists and subsequently distill their knowledge into a smaller efficient model, or using tricks like this:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Train specialist models on a subset of classes and keep one dustbin class for all the classes it doesn&#8217;t specialize in. The specialist should learn two things - a) is this sample in my specialized subset? b) the relative probabilities of all the classes in the subset.</p>
</li>
<li>
<p>After training, allow adjustment of the dustbin class logit to account for data enrichment. Ex. if 50% of samples are in the dustbin class in the training data while in real world they are 99%, we can adjust the logits with an additional log(50) or so to account for this.</p>
</li>
<li>
<p>Initialize with the specialist model with weights of a previously trained generalist model, use early stopping to prevent overfitting. Train on specialist classes.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Note: When combining specialist models, each with its own dustbin class, the model is not making a claim about the fact that all dustbin classes are equally probable but about the sum of probabilities of all dustbin classes.</p>
</div>
</div>
</div>]]></description><link>https://anshu92.github.io/blog/2019/09/10/Model-Distillation.html</link><guid isPermaLink="true">https://anshu92.github.io/blog/2019/09/10/Model-Distillation.html</guid><category><![CDATA[distillation]]></category><category><![CDATA[machine learning]]></category><dc:creator><![CDATA[Anshuman Sahoo]]></dc:creator><pubDate>Tue, 10 Sep 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[Variable Elimination Algorithm in Probabilistic Graph Inference]]></title><description><![CDATA[<div class="paragraph">
<p><strong><em>helpful links:</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/3.2.1-Inf-VE-Alg.pdf">summary of a section of course taught by Daphne Kohler, Stanford</a></p>
</li>
<li>
<p><a href="https://ermongroup.github.io/cs228-notes/inference/ve/" class="bare">https://ermongroup.github.io/cs228-notes/inference/ve/</a></p>
</li>
<li>
<p><a href="http://pgmlearning.herokuapp.com/vElimApp">Cool simulation of variable elimination</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Probabilistic graphical models represent rich representational structure to model processes. Inference then, is the process of getting insights from the structure in response to queries, and in the presence of certain evidence or observations.</p>
</div>
<div class="paragraph">
<p>Among these set of models, from Bayesian to Markov models where we have a set of observations say 'e' about some variable 'E', we have two kinds of queries  that are usually of interest -</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Conditional probability query</strong> - we are looking for a subset of all graph variables Y, i.e., we are trying to compute P(Y | E=e)</p>
</li>
<li>
<p><strong>Maximum a posteriori (MAP) query</strong> - we are looking for all values of Y, where Y is all variables except the evidence E that maximizes P(Y | E=e) (argmax_y p(y=y | E=e)</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Both these problems are theoretically NP-hard but that is the worst case situation. In general there are many strategies that make inference on PGMs tractable. There are many inference algorithms like Variable elimination, Message passing over a graph, belief propogation, variational approximation, Random sampling instantiations (Markov chain Monte Carlo, importance sampling).</p>
</div>
<div class="paragraph">
<p>Variable Elimination is a form of <strong>memoization/dynamic programming algorithm</strong>, where we compute factors as we try to eliminate variables along a graph.</p>
</div>
<div class="paragraph">
<p>In it&#8217;s simplest form, we can illustrate its working on a chain of variables (refer to slides provided in the link above). By marginalizing over a series of factors determined by order, which determines the run time of the algorithm, we can progressively eliminate 'local' variables from a factor. Finding the right order of variables is an NP-hard problem.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://image.slidesharecdn.com/lecture11xing-150527174444-lva1-app6892/95/lecture11-xing-17-638.jpg?cb=1432748719" alt="Example">
</div>
</div>
<div class="paragraph">
<p>Some useful strategies for determining the order of variables to eliminate include min-neighbors (Choose a variable with the fewest dependent variables), min-weight (Choose variables to minimize the product of the cardinalities of its dependent variables), min-fill (Choose vertices to minimize the size of the factor that will be added to the graph)</p>
</div>]]></description><link>https://anshu92.github.io/blog/2019/05/21/Variable-Elimination-Algorithm-in-Probabilistic-Graph-Inference.html</link><guid isPermaLink="true">https://anshu92.github.io/blog/2019/05/21/Variable-Elimination-Algorithm-in-Probabilistic-Graph-Inference.html</guid><category><![CDATA[probabilistic graphical models]]></category><category><![CDATA[pgm]]></category><category><![CDATA[inference]]></category><dc:creator><![CDATA[Anshuman Sahoo]]></dc:creator><pubDate>Tue, 21 May 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[ArcFace and other Geodesic Distance Loss Functions]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p><em>Paper:</em> ArcFace: Additive Angular Margin Loss for Deep Face Recognition <a href="https://arxiv.org/pdf/1801.07698.pdf" class="bare">https://arxiv.org/pdf/1801.07698.pdf</a></p>
</div>
<div class="paragraph">
<p>A challenge for DCNN (Deep Convolutional Neural Networks) based classification/recognition models is the closed-set nature of loss functions like Softmax as well as the fact that the learnt features are often not discriminative enough - and are often fooled by examples from the wild. A popular problem area that these particular issues are well highlighted is facial recognition, which is the topic of the paper above. However, I believe that the solutions applied to facial recognition here can be transferred to other areas that use features learnt through various DCNN architectures for some classification task.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_intra_class_compactness_and_inter_class_separability">Intra-class compactness and Inter-class separability</h3>
<div class="paragraph">
<p>In the design of loss functions that solve the challenges laid out above, two outcomes are desirable - one is that learnt features and weights of a particular class should lie close together in the vector space we are optimizing in (for Softmax, this is the Eucledian space). Center loss (which was the state of the art in facial recognition task before ArcFace) penalizes the distance between deep features and their corresponding class centres in the Euclidean space and hence attempts to achieve <strong>intra-class compactness</strong>. Another important loss function, SphereFace introduced the idea that the transformation matrix in the final fully connected layer can be used to represent class centers in angular space and penalize angles between deep features and weights in a multiplicative way. Both these losses helped improve the intra-class compactness but did not improve the inter-class separability. Eventually, this led to a popular line of research to incorporate margins in well-established loss functions in order to maximize inter-class separability.</p>
</div>
</div>
<div class="sect2">
<h3 id="_geodesic_distance_optimization">Geodesic Distance Optimization</h3>
<div class="paragraph">
<p>Geodesic distance between two points is the shortest distance between them on the a multi-dimensional surface (on a sphere, it is the arc between two points that forms part of the circle containing those points). There are 4 kinds of constraints we consider for this - Margin loss (insert a distance margin between samples and centers), Intra loss (decrease distance between sample and corresponding center), Inter loss (increase the distance between centers) and Triplet loss(insert margin between triplet samples &lt;<a href="https://en.wikipedia.org/wiki/Triplet_loss&gt;" class="bare">https://en.wikipedia.org/wiki/Triplet_loss&gt;</a>). Experiments have shown that Margin loss is the most effective strategy. By normalizing vectors so that the geodesic distance between them is the arc length on some n-dim sphere, we can reduce the problem to optimizing the angle between the vectors. In essence, this is what ArcFace aims at achieving.</p>
</div>
<table class="tableblock frame-all grid-all" style="width: 80%;">
<caption class="title">Table 1. Comparison of ArcFace to comparable loss functions</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Softmax</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Size of linear transformation matrix increases linearly with number of classes</p>
</li>
<li>
<p>Learned features are separable for closed set classification but not open-set recognition problems.</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Triplet</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Combinatorial explosion in number of image triplets</p>
</li>
<li>
<p>Difficulties in mining semi-hard samples, making effective training hard</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Center</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Updating the class centers during training is difficult as the number of classes increases</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">SphereFace</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Loss function requires lots of approximations which lead to unstable training. They had to hybridize with standard softmax to stabilize training.</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">CosFace</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Better than SphereFace but relieves the need for joint supervision from the softmax loss.</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">ArcFace</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Directly optimizes the geodesic distance margin by optimizing the angle and hence the arc in the normalized hypersphere. It is easy to implement and adds negligible complexity.</p>
</li>
</ul>
</div></div></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="_quick_derivation_from_softmax">Quick Derivation from Softmax</h3>
<div class="imageblock">
<div class="content">
<img src="https://cdn-images-1.medium.com/max/1600/1*lC5r61pId49Za7o0A1uvng.png" alt="Softmax">
</div>
</div>
<div class="paragraph">
<p>Here, N refers to batch size and n refers to class number. Next, we take the bias term out as equal to 0. This allows us to define the logit as -</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cdn-images-1.medium.com/max/1600/1*Rdqmp3_k3YhF6Wcii4aMTg.png" alt="Angle">
</div>
</div>
<div class="paragraph">
<p>Then we apply L2 normalization to the weight tensor to make it equal to 1. The feature tensor is also normalized and re-scaled to 's' which corresponds to the radius of the hypersphere. This gives us -</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cdn-images-1.medium.com/max/1600/1*lyJ1a8cd5mjnYmgj9tMV9g.png" alt="L2">
</div>
</div>
<div class="paragraph">
<p>The additive angular margin is then added to the angle between feature and weight to get -</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/L3.png" alt="L3">
</div>
</div>
<div class="paragraph">
<p>The paper also mentions combining SphereFace, CosFace and ArcFace into a unified metric -</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/CM1.png" alt="CM1">
</div>
</div>
<div class="paragraph">
<p>I have used this loss in my projects and it works really well in discriminating between subtly different classes.</p>
</div>
</div>]]></description><link>https://anshu92.github.io/blog/2019/05/16/Arc-Face-and-other-Geodesic-Distance-Loss-Functions.html</link><guid isPermaLink="true">https://anshu92.github.io/blog/2019/05/16/Arc-Face-and-other-Geodesic-Distance-Loss-Functions.html</guid><category><![CDATA[ML]]></category><category><![CDATA[machine learning]]></category><category><![CDATA[optimization]]></category><category><![CDATA[loss]]></category><category><![CDATA[computer vision]]></category><category><![CDATA[classification]]></category><category><![CDATA[summary]]></category><category><![CDATA[opinion]]></category><dc:creator><![CDATA[Anshuman Sahoo]]></dc:creator><pubDate>Thu, 16 May 2019 00:00:00 GMT</pubDate></item></channel></rss>