<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[hallucinations]]></title><description><![CDATA[by anshuman sahoo]]></description><link>https://anshu92.github.io/blog</link><image><url>/blog/images/cover/cover.jpg</url><title>hallucinations</title><link>https://anshu92.github.io/blog</link></image><generator>RSS for Node</generator><lastBuildDate>Tue, 10 Sep 2019 10:04:23 GMT</lastBuildDate><atom:link href="https://anshu92.github.io/blog/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Probabilistic Graphical Models - Message Passing in Cluster Graphs]]></title><description><![CDATA[<div class="paragraph">
<p><a href="https://www.coursera.org/learn/probabilistic-graphical-models-2-inference">Course link</a></p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/master/images/PGM%20MESSAGE%20PASSING%20IN%20CLUSTER%20GRAPHS.jpg" alt="Notes">
</div>
</div>]]></description><link>https://anshu92.github.io/blog/2019/07/23/Probabilistic-Graphical-Models-Message-Passing-in-Cluster-Graphs.html</link><guid isPermaLink="true">https://anshu92.github.io/blog/2019/07/23/Probabilistic-Graphical-Models-Message-Passing-in-Cluster-Graphs.html</guid><category><![CDATA[probabilistic graphical models]]></category><category><![CDATA[pgm]]></category><category><![CDATA[machine learning]]></category><dc:creator><![CDATA[Anshuman Sahoo]]></dc:creator><pubDate>Tue, 23 Jul 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[Probabilistic Graphical Models - Variable Elimination]]></title><description><![CDATA[<div class="paragraph">
<p><a href="https://www.coursera.org/learn/probabilistic-graphical-models-2-inference">Course link</a></p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/master/images/PGM%20VARIABLE%20ELIMINATION.jpg" alt="Notes">
</div>
</div>]]></description><link>https://anshu92.github.io/blog/2019/07/20/Probabilistic-Graphical-Models-Variable-Elimination.html</link><guid isPermaLink="true">https://anshu92.github.io/blog/2019/07/20/Probabilistic-Graphical-Models-Variable-Elimination.html</guid><category><![CDATA[probabilistic graphical models]]></category><category><![CDATA[pgm]]></category><category><![CDATA[machine learning]]></category><dc:creator><![CDATA[Anshuman Sahoo]]></dc:creator><pubDate>Sat, 20 Jul 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[Learning Data Augmentation Strategies for Object Detection]]></title><description><![CDATA[<div class="paragraph">
<p><a href="https://arxiv.org/pdf/1906.11172v1.pdf">Paper link</a></p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/master/images/Learning%20Data%20Augmentation%20Strategies%20for%20Object%20Detection.jpg" alt="Notes">
</div>
</div>]]></description><link>https://anshu92.github.io/blog/2019/07/20/Learning-Data-Augmentation-Strategies-for-Object-Detection.html</link><guid isPermaLink="true">https://anshu92.github.io/blog/2019/07/20/Learning-Data-Augmentation-Strategies-for-Object-Detection.html</guid><category><![CDATA[computer vision]]></category><category><![CDATA[machine learning]]></category><category><![CDATA[object detection]]></category><dc:creator><![CDATA[Anshuman Sahoo]]></dc:creator><pubDate>Sat, 20 Jul 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[5: Financial pearls of wisdom from The Intelligent Investor by Benjamin Graham]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Growing up in a predominantly fixed income culture, a lot of driven arguments on financial channels and sections of newspapers seemed to be made in bad faith, speculative and requiring a certain gambler&#8217;s expertise. And as a result, I have barely dipped a toe into the stressful art of investing by jumping on trendy bandwagons like cryptocurrency and robo-investing - staying away from any serious investing or building anything that can be termed as a 'portfolio'. However, recently I came across a <a href="https://www.kaggle.com/c/two-sigma-financial-news">kaggle competition</a> that aimed at predicting stock movements based on news articles. This peaked my interest in the game-like aspects of financial markets and quantitative finance. In order to gain a more balanced perspective on making informed financial decisions, I decided to read <em>The Intelligent Investor by Benjamin Graham</em>. Following is a commentary on salient ideas that I found interesting.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_investment_vs_speculation">Investment vs Speculation</h3>
<div class="ulist">
<ul>
<li>
<p>An investment operation is one which, upon thorough analysis, promises safety of principal and an adequate return.</p>
</li>
<li>
<p>Elements of investing -</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Thoroughly analyze a company and soundness of its businesses before buying stock.</p>
</li>
<li>
<p>Deliberately protect against serious losses.</p>
</li>
<li>
<p>Aspire to 'adequate', not extraordinary performance.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Speculation is buying stocks with a risk of loss. As Oscar Wilde puts it, a cynic knows the price of everything, but the value of nothing.</p>
</li>
<li>
<p>Speculating formulas that have failed in the past -</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><a href="https://en.wikipedia.org/wiki/January_effect">January effect</a></p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/James_O%27Shaughnessy">Do what works by James O&#8217;Shaugnessy</a></p>
</li>
<li>
<p><a href="https://jesse-livermore.com/the-foolish-four-and-trading-patterns/">Foolish Four</a></p>
</li>
</ol>
</div>
</li>
<li>
<p>Investing is less about predicting the behavior of the markets, and more about predicting and controlling your own behavior.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_defensive_vs_aggressive_investing">Defensive vs Aggressive Investing</h3>
<div class="paragraph">
<p>Defensive investing aims at minimizing the risk of losing principal while aggressive investing aims at maximizing return on investment.</p>
</div>
</div>
<div class="sect2">
<h3 id="_shorting">Shorting</h3>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Borrow shares from someone</p>
</li>
<li>
<p>Sell the borrowed shares at a higher price</p>
</li>
<li>
<p>Buy replacement shares at lower price</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="_inflation">Inflation</h3>
<div class="paragraph">
<p>Funny quote - <em>"Americans are getting stronger, 20 years ago, it took two people to carry ten dollars worth of groceries. Today, a five year old can do it." - Henry Youngman</em></p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Money Illusion</strong> - This is the phenomena when we prefer a nominal positive change in received value, even if the real/net value is negative. For example, we prefer a 2% pay raise when inflation is 4% over 2% pay cut when inflation is 0%.</p>
</li>
<li>
<p>Strategies to fight inflation</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>REITs - Real Estate Investment Trusts</p>
</li>
<li>
<p>TIPS - Treasury Inflated Protected Securities. Backed by US govt, values rises with inflation. The catch is that the income might become taxable by IRS when inflation rises and the value of the securities rises.</p>
</li>
</ol>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>This is how far I have gotten in the book (2 chapters). I will keep updating as I read on.</p>
</div>
</div>]]></description><link>https://anshu92.github.io/blog/2019/05/28/5-Financial-pearls-of-wisdom-from-The-Intelligent-Investor-by-Benjamin-Graham.html</link><guid isPermaLink="true">https://anshu92.github.io/blog/2019/05/28/5-Financial-pearls-of-wisdom-from-The-Intelligent-Investor-by-Benjamin-Graham.html</guid><category><![CDATA[finance]]></category><category><![CDATA[investing]]></category><dc:creator><![CDATA[Anshuman Sahoo]]></dc:creator><pubDate>Tue, 28 May 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[4: Variable Elimination Algorithm in Probabilistic Graph Inference]]></title><description><![CDATA[<div class="paragraph">
<p><strong><em>helpful links:</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/3.2.1-Inf-VE-Alg.pdf">summary of a section of course taught by Daphne Kohler, Stanford</a></p>
</li>
<li>
<p><a href="https://ermongroup.github.io/cs228-notes/inference/ve/" class="bare">https://ermongroup.github.io/cs228-notes/inference/ve/</a></p>
</li>
<li>
<p><a href="http://pgmlearning.herokuapp.com/vElimApp">Cool simulation of variable elimination</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Probabilistic graphical models represent rich representational structure to model processes. Inference then, is the process of getting insights from the structure in response to queries, and in the presence of certain evidence or observations.</p>
</div>
<div class="paragraph">
<p>Among these set of models, from Bayesian to Markov models where we have a set of observations say 'e' about some variable 'E', we have two kinds of queries  that are usually of interest -</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Conditional probability query</strong> - we are looking for a subset of all graph variables Y, i.e., we are trying to compute P(Y | E=e)</p>
</li>
<li>
<p><strong>Maximum a posteriori (MAP) query</strong> - we are looking for all values of Y, where Y is all variables except the evidence E that maximizes P(Y | E=e) (argmax_y p(y=y | E=e)</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Both these problems are theoretically NP-hard but that is the worst case situation. In general there are many strategies that make inference on PGMs tractable. There are many inference algorithms like Variable elimination, Message passing over a graph, belief propogation, variational approximation, Random sampling instantiations (Markov chain Monte Carlo, importance sampling).</p>
</div>
<div class="paragraph">
<p>Variable Elimination is a form of <strong>memoization/dynamic programming algorithm</strong>, where we compute factors as we try to eliminate variables along a graph.</p>
</div>
<div class="paragraph">
<p>In it&#8217;s simplest form, we can illustrate its working on a chain of variables (refer to slides provided in the link above). By marginalizing over a series of factors determined by order, which determines the run time of the algorithm, we can progressively eliminate 'local' variables from a factor. Finding the right order of variables is an NP-hard problem.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://image.slidesharecdn.com/lecture11xing-150527174444-lva1-app6892/95/lecture11-xing-17-638.jpg?cb=1432748719" alt="Example">
</div>
</div>
<div class="paragraph">
<p>Some useful strategies for determining the order of variables to eliminate include min-neighbors (Choose a variable with the fewest dependent variables), min-weight (Choose variables to minimize the product of the cardinalities of its dependent variables), min-fill (Choose vertices to minimize the size of the factor that will be added to the graph)</p>
</div>]]></description><link>https://anshu92.github.io/blog/2019/05/21/4-Variable-Elimination-Algorithm-in-Probabilistic-Graph-Inference.html</link><guid isPermaLink="true">https://anshu92.github.io/blog/2019/05/21/4-Variable-Elimination-Algorithm-in-Probabilistic-Graph-Inference.html</guid><category><![CDATA[probabilistic graphical models]]></category><category><![CDATA[pgm]]></category><category><![CDATA[inference]]></category><dc:creator><![CDATA[Anshuman Sahoo]]></dc:creator><pubDate>Tue, 21 May 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[3: Learning Representations through Causal Invariance]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p><a href="https://www.technologyreview.com/s/613502/deep-learning-could-reveal-why-the-world-works-the-way-it-does/?fbclid=IwAR2g29PKHoaqKU4P6mWcTwXKiCrm5QOJJ_-wCzzchC1QPpthVkOFLnG5W1w"><em>Summary of talk by Leon Bottou at ICLR 2019</em></a></p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_key_points">Key points</h3>
<div class="ulist">
<ul>
<li>
<p>Statistical problem is only a proxy to the real problem. Learning algorithms might be able to learn representations from data that satisfy the statistical conditions that occur with the problem, but they miss the point or miss some important correlation amidst other spurious correlations that it learns.</p>
</li>
<li>
<p>Nature doesn&#8217;t shuffle data, we do. Collection of data happens under varying environmental conditions with different experimental settings and biases. Earlier we used to curate data carefully to follow natural distribution. These days, we have large datasets that are randomly shuffled and assumed to be independent and identically distributed. The 'robust approach' involves interpolating different environments we have seen so far in correct proportions. However, interpolation is not enough and we need to be able to extrapolate to environments we have not seen before. In order to do this, we need to learn properties that are stable across environments - we need to learn a representation that can regress to the target while remaining invariant. Following this, we need to find variables that are relevant and ignore learning the spurious variables.</p>
</li>
<li>
<p>Adversarial Domain Adoptation - classifier that is invariant to changing environments.</p>
</li>
<li>
<p>Toy example - they use the idea of colored MNIST, where some numbers are colored in order to prove that machine learning algorithms will learn spurious correlations between color and classifications; which makes sense because of the data - but calls for design of algorithms that are robust to learning such relations.</p>
</li>
<li>
<p>Sidenote: <a href="https://www.youtube.com/watch?v=JTTiELgMyuM">Karush-Kuhn-Tucker conditions</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>A lot of interesting realizations about the design of representation learning algorithms follow from this. I had been stewing about ideas of modelling causality for time series problems for some time and it pleased me greatly to see a keynote speech at a major conference highlight this topic. It also highlights the fact that we shouldn&#8217;t lose sight of the reality of the problem we are solving or the physical phenomena or process that we are trying to teach a machine to represent in the purity, beauty and elegance of mathematical constructions that seem to perform and learn quantifiable statistical insights from data.</p>
</div>
</div>]]></description><link>https://anshu92.github.io/blog/2019/05/17/3-Learning-Representations-through-Causal-Invariance.html</link><guid isPermaLink="true">https://anshu92.github.io/blog/2019/05/17/3-Learning-Representations-through-Causal-Invariance.html</guid><category><![CDATA[iclr]]></category><category><![CDATA[summary]]></category><category><![CDATA[conference]]></category><category><![CDATA[machine learning]]></category><category><![CDATA[causality]]></category><dc:creator><![CDATA[Anshuman Sahoo]]></dc:creator><pubDate>Fri, 17 May 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[2: ArcFace and other Geodesic Distance Loss Functions]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p><em>Paper:</em> ArcFace: Additive Angular Margin Loss for Deep Face Recognition <a href="https://arxiv.org/pdf/1801.07698.pdf" class="bare">https://arxiv.org/pdf/1801.07698.pdf</a></p>
</div>
<div class="paragraph">
<p>A challenge for DCNN (Deep Convolutional Neural Networks) based classification/recognition models is the closed-set nature of loss functions like Softmax as well as the fact that the learnt features are often not discriminative enough - and are often fooled by examples from the wild. A popular problem area that these particular issues are well highlighted is facial recognition, which is the topic of the paper above. However, I believe that the solutions applied to facial recognition here can be transferred to other areas that use features learnt through various DCNN architectures for some classification task.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_intra_class_compactness_and_inter_class_separability">Intra-class compactness and Inter-class separability</h3>
<div class="paragraph">
<p>In the design of loss functions that solve the challenges laid out above, two outcomes are desirable - one is that learnt features and weights of a particular class should lie close together in the vector space we are optimizing in (for Softmax, this is the Eucledian space). Center loss (which was the state of the art in facial recognition task before ArcFace) penalizes the distance between deep features and their corresponding class centres in the Euclidean space and hence attempts to achieve <strong>intra-class compactness</strong>. Another important loss function, SphereFace introduced the idea that the transformation matrix in the final fully connected layer can be used to represent class centers in angular space and penalize angles between deep features and weights in a multiplicative way. Both these losses helped improve the intra-class compactness but did not improve the inter-class separability. Eventually, this led to a popular line of research to incorporate margins in well-established loss functions in order to maximize inter-class separability.</p>
</div>
</div>
<div class="sect2">
<h3 id="_geodesic_distance_optimization">Geodesic Distance Optimization</h3>
<div class="paragraph">
<p>Geodesic distance between two points is the shortest distance between them on the a multi-dimensional surface (on a sphere, it is the arc between two points that forms part of the circle containing those points). There are 4 kinds of constraints we consider for this - Margin loss (insert a distance margin between samples and centers), Intra loss (decrease distance between sample and corresponding center), Inter loss (increase the distance between centers) and Triplet loss(insert margin between triplet samples &lt;<a href="https://en.wikipedia.org/wiki/Triplet_loss&gt;" class="bare">https://en.wikipedia.org/wiki/Triplet_loss&gt;</a>). Experiments have shown that Margin loss is the most effective strategy. By normalizing vectors so that the geodesic distance between them is the arc length on some n-dim sphere, we can reduce the problem to optimizing the angle between the vectors. In essence, this is what ArcFace aims at achieving.</p>
</div>
<table class="tableblock frame-all grid-all" style="width: 80%;">
<caption class="title">Table 1. Comparison of ArcFace to comparable loss functions</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Softmax</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Size of linear transformation matrix increases linearly with number of classes</p>
</li>
<li>
<p>Learned features are separable for closed set classification but not open-set recognition problems.</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Triplet</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Combinatorial explosion in number of image triplets</p>
</li>
<li>
<p>Difficulties in mining semi-hard samples, making effective training hard</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Center</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Updating the class centers during training is difficult as the number of classes increases</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">SphereFace</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Loss function requires lots of approximations which lead to unstable training. They had to hybridize with standard softmax to stabilize training.</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">CosFace</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Better than SphereFace but relieves the need for joint supervision from the softmax loss.</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">ArcFace</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Directly optimizes the geodesic distance margin by optimizing the angle and hence the arc in the normalized hypersphere. It is easy to implement and adds negligible complexity.</p>
</li>
</ul>
</div></div></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="_quick_derivation_from_softmax">Quick Derivation from Softmax</h3>
<div class="imageblock">
<div class="content">
<img src="https://cdn-images-1.medium.com/max/1600/1*lC5r61pId49Za7o0A1uvng.png" alt="Softmax">
</div>
</div>
<div class="paragraph">
<p>Here, N refers to batch size and n refers to class number. Next, we take the bias term out as equal to 0. This allows us to define the logit as -</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cdn-images-1.medium.com/max/1600/1*Rdqmp3_k3YhF6Wcii4aMTg.png" alt="Angle">
</div>
</div>
<div class="paragraph">
<p>Then we apply L2 normalization to the weight tensor to make it equal to 1. The feature tensor is also normalized and re-scaled to 's' which corresponds to the radius of the hypersphere. This gives us -</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cdn-images-1.medium.com/max/1600/1*lyJ1a8cd5mjnYmgj9tMV9g.png" alt="L2">
</div>
</div>
<div class="paragraph">
<p>The additive angular margin is then added to the angle between feature and weight to get -</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/L3.png" alt="L3">
</div>
</div>
<div class="paragraph">
<p>The paper also mentions combining SphereFace, CosFace and ArcFace into a unified metric -</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/CM1.png" alt="CM1">
</div>
</div>
<div class="paragraph">
<p>I have used this loss in my projects and it works really well in discriminating between subtly different classes.</p>
</div>
</div>]]></description><link>https://anshu92.github.io/blog/2019/05/16/2-Arc-Face-and-other-Geodesic-Distance-Loss-Functions.html</link><guid isPermaLink="true">https://anshu92.github.io/blog/2019/05/16/2-Arc-Face-and-other-Geodesic-Distance-Loss-Functions.html</guid><category><![CDATA[ML]]></category><category><![CDATA[machine learning]]></category><category><![CDATA[optimization]]></category><category><![CDATA[loss]]></category><category><![CDATA[computer vision]]></category><category><![CDATA[classification]]></category><category><![CDATA[summary]]></category><category><![CDATA[opinion]]></category><dc:creator><![CDATA[Anshuman Sahoo]]></dc:creator><pubDate>Thu, 16 May 2019 00:00:00 GMT</pubDate></item></channel></rss>