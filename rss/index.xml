<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[hallucinations]]></title><description><![CDATA[by anshuman sahoo]]></description><link>https://anshu92.github.io/blog</link><image><url>/images/cover/coll.png</url><title>hallucinations</title><link>https://anshu92.github.io/blog</link></image><generator>RSS for Node</generator><lastBuildDate>Thu, 16 May 2019 03:58:11 GMT</lastBuildDate><atom:link href="https://anshu92.github.io/blog/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[1: About Bats and Artificial General Intelligence]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>In many works of fiction where an AI agent comes to life, often the first interaction is an emotional one. Vengeance, lust, fear or joy are tools used to convince us that the agent is human-like intelligent in its motives and actions. And our perceptions of what AGI should look like are polluted by the specificity of our perceptions regarding intelligent agents. A recent paper I read by Thomas Nagel titled <em>What is it like to be a bat?</em> ( <a href="https://en.wikipedia.org/wiki/What_Is_it_Like_to_Be_a_Bat%3F" class="bare">https://en.wikipedia.org/wiki/What_Is_it_Like_to_Be_a_Bat%3F</a> ) brought to the fore some points about such reductionist viewpoints that I felt were important.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_rallying_against_reductionism">Rallying against reductionism</h4>
<div class="paragraph">
<p>Nagel argues that depicting consciousness in terms of material explainations such as neurological signals leaves out important subjective elements. Like flavors of ice cream, there is something in the sense of being a particular organism - a subjective character of experience that cannot be ignored. Reductionist explanations are compatible with the absence of such subjectivity and hence lack something</p>
</div>
</div>
<div class="sect3">
<h4 id="_subjectivity_vs_objectivity">Subjectivity vs Objectivity</h4>
<div class="paragraph">
<p>BATS! Nagel chose bats for this analogy because they are close enough to us in the phylogenetic tree to be relevant but present a range of activity and sensory apparatus that are very different from us. Bats use sonar/echo location as their primary sensory system. Now we objectively can understand how they send clicks out and their brains correlate the expected return signals to volume, depth and distance. But subjectively, their experience is definitely unlike anything we can imagine. So can any method allow us to extrapolate the inner life of a bat? Our imagination is limited by our experience since it is our perception of reality that provides the basic material to it. I can imagine webbed arms, poor vision and hanging upside down from the ceiling. That <em>might</em> be good enough to tell me how a bat behaves but not what it is like to be a bat. In a similar way, a Martian with a different but equal set of intellectual abilities might be able to objectively understand the concept of lighting. However, when they inevitably cut open our heads - upon examining our neurons, would they be able to understand the <em>subjectively</em> different function lightning/electricity plays in our brains? All this is to highlight the upper limits of perceptual abilities and the fact that ignoring this is doing a disservice to ever creating any form of 'AI' or even understanding what manufactured intelligence might look like beyond those crafted to appease our narrow imaginative limits. It is a crude form of cognitive dissonance that we are foolng ourselves into.</p>
</div>
</div>
<div class="sect3">
<h4 id="_facts_vs_conceptual_schemes">Facts vs conceptual schemes</h4>
<div class="paragraph">
<p>Another area of caution is to accept that there are humanly inaccesible facts, and in our inability to know what we don&#8217;t know, we create conceptual schemes that 'fit'. Objectivity is simply reducing our dependence on species/situation specific points towards the object of investigation. However, greater objectivity takes us farther from the real nature of the phenomena.</p>
</div>
<div class="paragraph">
<p>And so, if the quest for AGI is to be a tractable one, we must first construct a better empathy towards alien systems of intelligence and perhaps understand processes we deem unintellectual in the glow of newfound humility.</p>
</div>
</div>]]></description><link>https://anshu92.github.io/blog/2019/05/15/1-About-Bats-and-Artificial-General-Intelligence.html</link><guid isPermaLink="true">https://anshu92.github.io/blog/2019/05/15/1-About-Bats-and-Artificial-General-Intelligence.html</guid><category><![CDATA[artificial general intelligence]]></category><category><![CDATA[ ai]]></category><category><![CDATA[ AI]]></category><category><![CDATA[ philosophy]]></category><category><![CDATA[ consciousness]]></category><dc:creator><![CDATA[Anshuman Sahoo]]></dc:creator><pubDate>Wed, 15 May 2019 00:00:00 GMT</pubDate></item></channel></rss>