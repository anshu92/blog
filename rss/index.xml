<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[hallucinations]]></title><description><![CDATA[by anshuman sahoo]]></description><link>https://anshu92.github.io/blog</link><image><url>/blog/images/cover/cover.jpg</url><title>hallucinations</title><link>https://anshu92.github.io/blog</link></image><generator>RSS for Node</generator><lastBuildDate>Thu, 12 Sep 2019 04:11:20 GMT</lastBuildDate><atom:link href="https://anshu92.github.io/blog/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Model Distillation]]></title><description><![CDATA[<div class="paragraph lead">
<p><strong>How most of the information in large models can be transferred to a small model using soft targets, and what that says about the relationship of information to the dynamics of learning.</strong></p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/carolien-van-oijen-GRlRHqEqZTc-unsplash.jpg" alt="Bee">
</div>
</div>
<hr>
<table class="tableblock frame-all grid-all" style="width: 50%;">
<caption class="title">Table 1. Recent NLP language models</caption>
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>model</strong></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><strong>parameters</strong></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">BERT-base</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">110 million</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">BERT-large</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">340 million</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Facebook XLM</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">665 million</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">OpenAI GPT-2</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">774 million</p></td>
</tr>
</tbody>
</table>
<hr>
<div class="paragraph">
<p>The upward trend of model sizes raises hurdles for the application of these models in the shape of computation and scalability, and requires decisions on the trade-off between accuracy, potency and deployability. Following reading <a href="http://www.nlp.town/blog/distilling-bert/">this blog post</a>, I decided to try to summarize (distill) the concept of model distillation, and talk about why this struck me as such an interesting idea.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/distill.png" alt="Model Distillation">
</div>
<div class="title">Figure 1. Model Distillation. graphic by Anshuman Sahoo</div>
</div>
<hr>
<div class="paragraph">
<p>As shown above, the method of distillation relies on the idea of <strong><em>soft targets</em></strong> and <strong><em>softmax temperature</em></strong>.</p>
</div>
<hr>
<div class="imageblock" style="text-align: center">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/tempsoftmax.png" alt="Model Distillation">
</div>
<div class="title">Figure 2. Softmax Function with Temperature</div>
</div>
<hr>
<div class="paragraph">
<p><a href="https://arxiv.org/pdf/1503.02531.pdf">Hinton et al., 2015</a> introduced the concept of <strong>"softmax temperature"</strong>. The probability p<sub>i</sub> of class i is calculated from the logits z. T is the temperature parameter - as you can see, setting T = 1 gives us the standard softmax function. In the paper, they also found that if the ground truth labels are available, it improves the model if the loss to optimize includes training on ground truth labels in addition to the soft target outputs of the teacher model(as shown in the figure, student loss is added to distillation loss).</p>
</div>
<div class="paragraph">
<p>On a philosophical line of thought, the paper presents an analogy of some insects in nature that optimize based on their stage-of-life requirement - larval forms to extract nutrition from environment and an adult form for travellig and reproduction; whereas cumbersome, large and often ensemble models remain the same during training and deployment. A conceptual block that may explain our hesitation of modifying trained models is our assumption that changing learned parameters may lead to loss of knowledge. Another assumption made is that the optimizing an objective function for the training set is close to optimizing the user&#8217;s true objective - generalizing well to unseen data. However, ideal generalization to the user objective requires knowledge that might not be available in the training data.</p>
</div>
<div class="paragraph">
<p>Using the output of the teacher model allows the student model to learn not just about the correct class(as it would in the case of hard labels) but also the relative incorrectness of the other classes. For instance, knowledge that a cat is closer to a tiger than a carrot is valuable for generalization of the model. Quoting from the paper -</p>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>When the soft targets have high entropy, they provide much more information per training case than hard targets and much less variance in the gradient between training cases, so the small model can often be trained on much
less data than the original cumbersome model and using a much higher learning rate.</p>
</div>
</blockquote>
<div class="attribution">
&#8212; Hinton et al.<br>
<cite>2015</cite>
</div>
</div>]]></description><link>https://anshu92.github.io/blog/2019/09/10/Model-Distillation.html</link><guid isPermaLink="true">https://anshu92.github.io/blog/2019/09/10/Model-Distillation.html</guid><category><![CDATA[distillation]]></category><category><![CDATA[machine learning]]></category><dc:creator><![CDATA[Anshuman Sahoo]]></dc:creator><pubDate>Tue, 10 Sep 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[Variable Elimination Algorithm in Probabilistic Graph Inference]]></title><description><![CDATA[<div class="paragraph">
<p><strong><em>helpful links:</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/3.2.1-Inf-VE-Alg.pdf">summary of a section of course taught by Daphne Kohler, Stanford</a></p>
</li>
<li>
<p><a href="https://ermongroup.github.io/cs228-notes/inference/ve/" class="bare">https://ermongroup.github.io/cs228-notes/inference/ve/</a></p>
</li>
<li>
<p><a href="http://pgmlearning.herokuapp.com/vElimApp">Cool simulation of variable elimination</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Probabilistic graphical models represent rich representational structure to model processes. Inference then, is the process of getting insights from the structure in response to queries, and in the presence of certain evidence or observations.</p>
</div>
<div class="paragraph">
<p>Among these set of models, from Bayesian to Markov models where we have a set of observations say 'e' about some variable 'E', we have two kinds of queries  that are usually of interest -</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Conditional probability query</strong> - we are looking for a subset of all graph variables Y, i.e., we are trying to compute P(Y | E=e)</p>
</li>
<li>
<p><strong>Maximum a posteriori (MAP) query</strong> - we are looking for all values of Y, where Y is all variables except the evidence E that maximizes P(Y | E=e) (argmax_y p(y=y | E=e)</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Both these problems are theoretically NP-hard but that is the worst case situation. In general there are many strategies that make inference on PGMs tractable. There are many inference algorithms like Variable elimination, Message passing over a graph, belief propogation, variational approximation, Random sampling instantiations (Markov chain Monte Carlo, importance sampling).</p>
</div>
<div class="paragraph">
<p>Variable Elimination is a form of <strong>memoization/dynamic programming algorithm</strong>, where we compute factors as we try to eliminate variables along a graph.</p>
</div>
<div class="paragraph">
<p>In it&#8217;s simplest form, we can illustrate its working on a chain of variables (refer to slides provided in the link above). By marginalizing over a series of factors determined by order, which determines the run time of the algorithm, we can progressively eliminate 'local' variables from a factor. Finding the right order of variables is an NP-hard problem.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://image.slidesharecdn.com/lecture11xing-150527174444-lva1-app6892/95/lecture11-xing-17-638.jpg?cb=1432748719" alt="Example">
</div>
</div>
<div class="paragraph">
<p>Some useful strategies for determining the order of variables to eliminate include min-neighbors (Choose a variable with the fewest dependent variables), min-weight (Choose variables to minimize the product of the cardinalities of its dependent variables), min-fill (Choose vertices to minimize the size of the factor that will be added to the graph)</p>
</div>]]></description><link>https://anshu92.github.io/blog/2019/05/21/Variable-Elimination-Algorithm-in-Probabilistic-Graph-Inference.html</link><guid isPermaLink="true">https://anshu92.github.io/blog/2019/05/21/Variable-Elimination-Algorithm-in-Probabilistic-Graph-Inference.html</guid><category><![CDATA[probabilistic graphical models]]></category><category><![CDATA[pgm]]></category><category><![CDATA[inference]]></category><dc:creator><![CDATA[Anshuman Sahoo]]></dc:creator><pubDate>Tue, 21 May 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[ArcFace and other Geodesic Distance Loss Functions]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p><em>Paper:</em> ArcFace: Additive Angular Margin Loss for Deep Face Recognition <a href="https://arxiv.org/pdf/1801.07698.pdf" class="bare">https://arxiv.org/pdf/1801.07698.pdf</a></p>
</div>
<div class="paragraph">
<p>A challenge for DCNN (Deep Convolutional Neural Networks) based classification/recognition models is the closed-set nature of loss functions like Softmax as well as the fact that the learnt features are often not discriminative enough - and are often fooled by examples from the wild. A popular problem area that these particular issues are well highlighted is facial recognition, which is the topic of the paper above. However, I believe that the solutions applied to facial recognition here can be transferred to other areas that use features learnt through various DCNN architectures for some classification task.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_intra_class_compactness_and_inter_class_separability">Intra-class compactness and Inter-class separability</h3>
<div class="paragraph">
<p>In the design of loss functions that solve the challenges laid out above, two outcomes are desirable - one is that learnt features and weights of a particular class should lie close together in the vector space we are optimizing in (for Softmax, this is the Eucledian space). Center loss (which was the state of the art in facial recognition task before ArcFace) penalizes the distance between deep features and their corresponding class centres in the Euclidean space and hence attempts to achieve <strong>intra-class compactness</strong>. Another important loss function, SphereFace introduced the idea that the transformation matrix in the final fully connected layer can be used to represent class centers in angular space and penalize angles between deep features and weights in a multiplicative way. Both these losses helped improve the intra-class compactness but did not improve the inter-class separability. Eventually, this led to a popular line of research to incorporate margins in well-established loss functions in order to maximize inter-class separability.</p>
</div>
</div>
<div class="sect2">
<h3 id="_geodesic_distance_optimization">Geodesic Distance Optimization</h3>
<div class="paragraph">
<p>Geodesic distance between two points is the shortest distance between them on the a multi-dimensional surface (on a sphere, it is the arc between two points that forms part of the circle containing those points). There are 4 kinds of constraints we consider for this - Margin loss (insert a distance margin between samples and centers), Intra loss (decrease distance between sample and corresponding center), Inter loss (increase the distance between centers) and Triplet loss(insert margin between triplet samples &lt;<a href="https://en.wikipedia.org/wiki/Triplet_loss&gt;" class="bare">https://en.wikipedia.org/wiki/Triplet_loss&gt;</a>). Experiments have shown that Margin loss is the most effective strategy. By normalizing vectors so that the geodesic distance between them is the arc length on some n-dim sphere, we can reduce the problem to optimizing the angle between the vectors. In essence, this is what ArcFace aims at achieving.</p>
</div>
<table class="tableblock frame-all grid-all" style="width: 80%;">
<caption class="title">Table 1. Comparison of ArcFace to comparable loss functions</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Softmax</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Size of linear transformation matrix increases linearly with number of classes</p>
</li>
<li>
<p>Learned features are separable for closed set classification but not open-set recognition problems.</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Triplet</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Combinatorial explosion in number of image triplets</p>
</li>
<li>
<p>Difficulties in mining semi-hard samples, making effective training hard</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Center</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Updating the class centers during training is difficult as the number of classes increases</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">SphereFace</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Loss function requires lots of approximations which lead to unstable training. They had to hybridize with standard softmax to stabilize training.</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">CosFace</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Better than SphereFace but relieves the need for joint supervision from the softmax loss.</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">ArcFace</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Directly optimizes the geodesic distance margin by optimizing the angle and hence the arc in the normalized hypersphere. It is easy to implement and adds negligible complexity.</p>
</li>
</ul>
</div></div></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="_quick_derivation_from_softmax">Quick Derivation from Softmax</h3>
<div class="imageblock">
<div class="content">
<img src="https://cdn-images-1.medium.com/max/1600/1*lC5r61pId49Za7o0A1uvng.png" alt="Softmax">
</div>
</div>
<div class="paragraph">
<p>Here, N refers to batch size and n refers to class number. Next, we take the bias term out as equal to 0. This allows us to define the logit as -</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cdn-images-1.medium.com/max/1600/1*Rdqmp3_k3YhF6Wcii4aMTg.png" alt="Angle">
</div>
</div>
<div class="paragraph">
<p>Then we apply L2 normalization to the weight tensor to make it equal to 1. The feature tensor is also normalized and re-scaled to 's' which corresponds to the radius of the hypersphere. This gives us -</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cdn-images-1.medium.com/max/1600/1*lyJ1a8cd5mjnYmgj9tMV9g.png" alt="L2">
</div>
</div>
<div class="paragraph">
<p>The additive angular margin is then added to the angle between feature and weight to get -</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/L3.png" alt="L3">
</div>
</div>
<div class="paragraph">
<p>The paper also mentions combining SphereFace, CosFace and ArcFace into a unified metric -</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/CM1.png" alt="CM1">
</div>
</div>
<div class="paragraph">
<p>I have used this loss in my projects and it works really well in discriminating between subtly different classes.</p>
</div>
</div>]]></description><link>https://anshu92.github.io/blog/2019/05/16/Arc-Face-and-other-Geodesic-Distance-Loss-Functions.html</link><guid isPermaLink="true">https://anshu92.github.io/blog/2019/05/16/Arc-Face-and-other-Geodesic-Distance-Loss-Functions.html</guid><category><![CDATA[ML]]></category><category><![CDATA[machine learning]]></category><category><![CDATA[optimization]]></category><category><![CDATA[loss]]></category><category><![CDATA[computer vision]]></category><category><![CDATA[classification]]></category><category><![CDATA[summary]]></category><category><![CDATA[opinion]]></category><dc:creator><![CDATA[Anshuman Sahoo]]></dc:creator><pubDate>Thu, 16 May 2019 00:00:00 GMT</pubDate></item></channel></rss>