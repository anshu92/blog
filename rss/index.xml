<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[hallucinations]]></title><description><![CDATA[by anshuman sahoo]]></description><link>https://anshu92.github.io/blog</link><image><url>/images/cover/winterwonderland.jpg</url><title>hallucinations</title><link>https://anshu92.github.io/blog</link></image><generator>RSS for Node</generator><lastBuildDate>Sat, 18 May 2019 06:00:58 GMT</lastBuildDate><atom:link href="https://anshu92.github.io/blog/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[3: Learning Representations through Causal Invariance]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p><a href="https://www.technologyreview.com/s/613502/deep-learning-could-reveal-why-the-world-works-the-way-it-does/?fbclid=IwAR2g29PKHoaqKU4P6mWcTwXKiCrm5QOJJ_-wCzzchC1QPpthVkOFLnG5W1w"><em>Summary of talk by Leon Bottou at ICLR 2019</em></a></p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_key_points">Key points</h3>
<div class="ulist">
<ul>
<li>
<p>Statistical problem is only a proxy to the real problem. Learning algorithms might be able to learn representations from data that satisfy the statistical conditions that occur with the problem, but they miss the point or miss some important correlation amidst other spurious correlations that it learns.</p>
</li>
<li>
<p>Nature doesn&#8217;t shuffle data, we do. Collection of data happens under varying environmental conditions with different experimental settings and biases. Earlier we used to curate data carefully to follow natural distribution. These days, we have large datasets that are randomly shuffled and assumed to be independent and identically distributed. The 'robust approach' involves interpolating different environments we have seen so far in correct proportions. However, interpolation is not enough and we need to be able to extrapolate to environments we have not seen before. In order to do this, we need to learn properties that are stable across environments - we need to learn a representation that can regress to the target while remaining invariant. Following this, we need to find variables that are relevant and ignore learning the spurious variables.</p>
</li>
<li>
<p>Adversarial Domain Adoptation - classifier that is invariant to changing environments.</p>
</li>
<li>
<p>Toy example - they use the idea of colored MNIST, where some numbers are colored in order to prove that machine learning algorithms will learn spurious correlations between color and classifications; which makes sense because of the data - but calls for design of algorithms that are robust to learning such relations.</p>
</li>
<li>
<p>Sidenote: <a href="https://www.youtube.com/watch?v=JTTiELgMyuM">Karush-Kuhn-Tucker conditions</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>A lot of interesting realizations about the design of representation learning algorithms follow from this. I had been stewing about ideas of modelling causality for time series problems for some time and it pleased me greatly to see a keynote speech at a major conference highlight this topic. It also highlights the fact that we shouldn&#8217;t lose sight of the reality of the problem we are solving or the physical phenomena or process that we are trying to teach a machine to represent in the purity, beauty and elegance of mathematical constructions that seem to perform and learn quantifiable statistical insights from data.</p>
</div>
</div>]]></description><link>https://anshu92.github.io/blog/2019/05/17/3-Learning-Representations-through-Causal-Invariance.html</link><guid isPermaLink="true">https://anshu92.github.io/blog/2019/05/17/3-Learning-Representations-through-Causal-Invariance.html</guid><category><![CDATA[iclr]]></category><category><![CDATA[ summary]]></category><category><![CDATA[ conference]]></category><category><![CDATA[ machine learning]]></category><category><![CDATA[ causality]]></category><dc:creator><![CDATA[Anshuman Sahoo]]></dc:creator><pubDate>Fri, 17 May 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[2: ArcFace and other Geodesic Distance Loss Functions]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p><em>Paper:</em> ArcFace: Additive Angular Margin Loss for Deep Face Recognition <a href="https://arxiv.org/pdf/1801.07698.pdf" class="bare">https://arxiv.org/pdf/1801.07698.pdf</a></p>
</div>
<div class="paragraph">
<p>A challenge for DCNN (Deep Convolutional Neural Networks) based classification/recognition models is the closed-set nature of loss functions like Softmax as well as the fact that the learnt features are often not discriminative enough - and are often fooled by examples from the wild. A popular problem area that these particular issues are well highlighted is facial recognition, which is the topic of the paper above. However, I believe that the solutions applied to facial recognition here can be transferred to other areas that use features learnt through various DCNN architectures for some classification task.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_intra_class_compactness_and_inter_class_separability">Intra-class compactness and Inter-class separability</h3>
<div class="paragraph">
<p>In the design of loss functions that solve the challenges laid out above, two outcomes are desirable - one is that learnt features and weights of a particular class should lie close together in the vector space we are optimizing in (for Softmax, this is the Eucledian space). Center loss (which was the state of the art in facial recognition task before ArcFace) penalizes the distance between deep features and their corresponding class centres in the Euclidean space and hence attempts to achieve <strong>intra-class compactness</strong>. Another important loss function, SphereFace introduced the idea that the transformation matrix in the final fully connected layer can be used to represent class centers in angular space and penalize angles between deep features and weights in a multiplicative way. Both these losses helped improve the intra-class compactness but did not improve the inter-class separability. Eventually, this led to a popular line of research to incorporate margins in well-established loss functions in order to maximize inter-class separability.</p>
</div>
</div>
<div class="sect2">
<h3 id="_geodesic_distance_optimization">Geodesic Distance Optimization</h3>
<div class="paragraph">
<p>Geodesic distance between two points is the shortest distance between them on the a multi-dimensional surface (on a sphere, it is the arc between two points that forms part of the circle containing those points). There are 4 kinds of constraints we consider for this - Margin loss (insert a distance margin between samples and centers), Intra loss (decrease distance between sample and corresponding center), Inter loss (increase the distance between centers) and Triplet loss(insert margin between triplet samples &lt;<a href="https://en.wikipedia.org/wiki/Triplet_loss&gt;" class="bare">https://en.wikipedia.org/wiki/Triplet_loss&gt;</a>). Experiments have shown that Margin loss is the most effective strategy. By normalizing vectors so that the geodesic distance between them is the arc length on some n-dim sphere, we can reduce the problem to optimizing the angle between the vectors. In essence, this is what ArcFace aims at achieving.</p>
</div>
<table class="tableblock frame-all grid-all" style="width: 80%;">
<caption class="title">Table 1. Comparison of ArcFace to comparable loss functions</caption>
<colgroup>
<col style="width: 20%;">
<col style="width: 80%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Softmax</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Size of linear transformation matrix increases linearly with number of classes</p>
</li>
<li>
<p>Learned features are separable for closed set classification but not open-set recognition problems.</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Triplet</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Combinatorial explosion in number of image triplets</p>
</li>
<li>
<p>Difficulties in mining semi-hard samples, making effective training hard</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Center</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Updating the class centers during training is difficult as the number of classes increases</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">SphereFace</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Loss function requires lots of approximations which lead to unstable training. They had to hybridize with standard softmax to stabilize training.</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">CosFace</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Better than SphereFace but relieves the need for joint supervision from the softmax loss.</p>
</li>
</ul>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">ArcFace</p></td>
<td class="tableblock halign-left valign-top"><div><div class="ulist">
<ul>
<li>
<p>Directly optimizes the geodesic distance margin by optimizing the angle and hence the arc in the normalized hypersphere. It is easy to implement and adds negligible complexity.</p>
</li>
</ul>
</div></div></td>
</tr>
</tbody>
</table>
</div>
<div class="sect2">
<h3 id="_quick_derivation_from_softmax">Quick Derivation from Softmax</h3>
<div class="imageblock">
<div class="content">
<img src="https://cdn-images-1.medium.com/max/1600/1*lC5r61pId49Za7o0A1uvng.png" alt="Softmax">
</div>
</div>
<div class="paragraph">
<p>Here, N refers to batch size and n refers to class number. Next, we take the bias term out as equal to 0. This allows us to define the logit as -</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cdn-images-1.medium.com/max/1600/1*Rdqmp3_k3YhF6Wcii4aMTg.png" alt="Angle">
</div>
</div>
<div class="paragraph">
<p>Then we apply L2 normalization to the weight tensor to make it equal to 1. The feature tensor is also normalized and re-scaled to 's' which corresponds to the radius of the hypersphere. This gives us -</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cdn-images-1.medium.com/max/1600/1*lyJ1a8cd5mjnYmgj9tMV9g.png" alt="L2">
</div>
</div>
<div class="paragraph">
<p>The additive angular margin is then added to the angle between feature and weight to get -</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/L3.png" alt="L3">
</div>
</div>
<div class="paragraph">
<p>The paper also mentions combining SphereFace, CosFace and ArcFace into a unified metric -</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://github.com/anshu92/blog/raw/gh-pages/images/CM1.png" alt="CM1">
</div>
</div>
<div class="paragraph">
<p>I have used this loss in my projects and it works really well in discriminating between subtly different classes.</p>
</div>
</div>]]></description><link>https://anshu92.github.io/blog/2019/05/16/2-Arc-Face-and-other-Geodesic-Distance-Loss-Functions.html</link><guid isPermaLink="true">https://anshu92.github.io/blog/2019/05/16/2-Arc-Face-and-other-Geodesic-Distance-Loss-Functions.html</guid><category><![CDATA[ML]]></category><category><![CDATA[ machine learning]]></category><category><![CDATA[ optimization]]></category><category><![CDATA[ loss]]></category><category><![CDATA[ computer vision]]></category><category><![CDATA[ classification]]></category><category><![CDATA[ summary]]></category><category><![CDATA[ opinion]]></category><dc:creator><![CDATA[Anshuman Sahoo]]></dc:creator><pubDate>Thu, 16 May 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[1: About Bats and Artificial General Intelligence]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>In many works of fiction where an AI agent comes to life, often the first interaction is an emotional one. Vengeance, lust, fear or joy are tools used to convince us that the agent is human-like intelligent in its motives and actions. And our perceptions of what AGI should look like are polluted by the specificity of our perceptions regarding intelligent agents. A paper I read recently by Thomas Nagel titled <em>What is it like to be a bat?</em> ( <a href="https://en.wikipedia.org/wiki/What_Is_it_Like_to_Be_a_Bat%3F" class="bare">https://en.wikipedia.org/wiki/What_Is_it_Like_to_Be_a_Bat%3F</a> ) brought to the fore some points about such reductionist viewpoints that I felt were important.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_rallying_against_reductionism">Rallying against reductionism</h4>
<div class="paragraph">
<p>Nagel argues that depicting consciousness in terms of material explainations such as neurological signals leaves out important subjective elements. Like flavors of ice cream, there is something in the sense of being a particular organism - a subjective character of experience that cannot be ignored. Reductionist explanations are compatible with the absence of such subjectivity and hence lack something</p>
</div>
</div>
<div class="sect3">
<h4 id="_subjectivity_vs_objectivity">Subjectivity vs Objectivity</h4>
<div class="paragraph">
<p>BATS! Nagel chose bats for this analogy because they are close enough to us in the phylogenetic tree to be relevant but present a range of activity and sensory apparatus that are very different from us. Bats use sonar/echo location as their primary sensory system. Now we objectively can understand how they send clicks out and their brains correlate the expected return signals to volume, depth and distance. But subjectively, their experience is definitely unlike anything we can imagine. So can any method allow us to extrapolate the inner life of a bat? Our imagination is limited by our experience since it is our perception of reality that provides the basic material to it. I can imagine webbed arms, poor vision and hanging upside down from the ceiling. That <em>might</em> be good enough to tell me how a bat behaves but not what it is like to be a bat. In a similar way, a Martian with a different but equal set of intellectual abilities might be able to objectively understand the concept of lighting. However, when they inevitably cut open our heads - upon examining our neurons, would they be able to understand the <em>subjectively</em> different function lightning/electricity plays in our brains? All this is to highlight the upper limits of perceptual abilities and the fact that ignoring this is doing a disservice to ever creating any form of 'AI' or even understanding what manufactured intelligence might look like beyond those crafted to appease our narrow imaginative limits. It is a crude form of cognitive dissonance that we are foolng ourselves into.</p>
</div>
</div>
<div class="sect3">
<h4 id="_facts_vs_conceptual_schemes">Facts vs conceptual schemes</h4>
<div class="paragraph">
<p>Another area of caution is to accept that there are humanly inaccesible facts, and in our inability to know what we don&#8217;t know, we create conceptual schemes that 'fit'. Objectivity is simply reducing our dependence on species/situation specific points towards the object of investigation. However, greater objectivity takes us farther from the real nature of the phenomena.</p>
</div>
<div class="paragraph">
<p>And so, if the quest for AGI is to be a tractable one, we must first construct a better empathy towards alien systems of intelligence and perhaps understand processes we deem unintellectual in the glow of newfound humility.</p>
</div>
</div>]]></description><link>https://anshu92.github.io/blog/2019/05/15/1-About-Bats-and-Artificial-General-Intelligence.html</link><guid isPermaLink="true">https://anshu92.github.io/blog/2019/05/15/1-About-Bats-and-Artificial-General-Intelligence.html</guid><category><![CDATA[artificial general intelligence]]></category><category><![CDATA[ ai]]></category><category><![CDATA[ AI]]></category><category><![CDATA[ philosophy]]></category><category><![CDATA[ consciousness]]></category><dc:creator><![CDATA[Anshuman Sahoo]]></dc:creator><pubDate>Wed, 15 May 2019 00:00:00 GMT</pubDate></item></channel></rss>